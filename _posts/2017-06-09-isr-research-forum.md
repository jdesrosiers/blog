---
layout: post
title:  "ISR Research Forum"
date:   2017-06-09 12:00:00
tags: [research]
---

Last week I went to the Institute for Software Research (ISR) Research Forum at UC Irvine.  It’s a little conference held each year to highlight the research that the faculty and grad students have been working on the past year.  They also bring in people from industry or other universities to talk about the research they are doing.  There were three topics that I found especially thought provoking: code visualization, clone detection, and cognitive components.

The first was Jim Jones’s summary of his and his grad student’s work in code visualization.  Jim developed a tool called Tarantula that visualizes the test results of a system under test.  It will display all of the code with lines of code associated with passing tests are green and lines of code associated with failing tests are red.  However, most lines of code are tested by more than one test.  Some of these tests pass and some fail.  In this case, the color will be some shade of yellow or orange depending on if most tests pass or most fail.

In the example he showed, most of the code was green with some yellow, but there were a couple patches of red.  That begs the question of whether these patches are failing for different reasons or if they are related.  Later Jim’s team created a visualization tool that converts the Tarantula data into nodes on a graph.  The graph pulls nodes of code that get executed together into groups.  Using the same example, we saw that all the red nodes were pulled together indicating that they are actually related.

The purpose of this research is to improve the feedback of test results, but something else stood out to me.  The visualization revealed a lack of cohesiveness in the code.  The red code was clearly related, but it was spread across the code base.  It occurs to me that independent of testing results, this visualization tool might be useful to get feedback on code quality.  The problem is that when the code is in graph form, you lose understanding of what code the nodes refer to.  We only knew the code in the previous example because it was red.  I can imagine a different coloring scheme that identifies how dispersed a cluster of nodes is in the codebase.  We could then select a highly incohesive cluster and go back to the Tarantula-like view of the code to analyse how it could be improved.

The next topic was Crista Lopes’s talk on her team’s work on analyzing code on a massive scale.  They downloaded a snapshot of all public repositories on Github and have been running analysis on that massive set of data.  One of the things they have been studying is code duplication (excluding forks).  It turns out that a very large portion of Github is people committing their dependencies along with their code.  They did an analysis of code duplication per repository per language.  It seems that node.js developers are the worst offenders.  The percentage of duplication in JavaScript projects is in the high 90s.  Apparently there is an epidemic of people committing their `node_modules` directories instead of letting npm manage directories.  I’m not sure what to take away from that result, but it was interesting.

That study looked for exact file matches, but they have also been developing tools for more fuzzy clone detection.  That got me thinking about a particular type of cloning that they had not studied.  It is possible to write the same code in many different ways yet be identical in functionality.  One example is an algorithm written recursively and another written iteratively.  Current clone detection tools aren’t capable of recognizing something like this as duplication and we would miss the opportunity to identify an opportunity for reuse.  Something like this would probably require some level of AI, but it could be quite useful.

The third topic I wanted to discuss was from a guest keynote speaker Peri Tarr from IBM.  Peri talked about Cognitive Software Engineering.  I felt like this talk was more questions than answers, but I think that was point.  Machine learning is becoming more and more prevalent in the systems we use every day, yet we know very little about these cognitive components fit into our software development process.  Peri points out that the way we write software is very different that the way we train cognitive components and requires a different way of approaching these tasks.

She expects that the developers of these cognitive components will likely be a separate group because of these differences.  I think that is a “Software Engineer’s” way to look at the problem.  SE tends to look to ways to separate and silo responsibilities.  However, this is more and more and agile industry.  People (and more so companies) are slow to change, but most are at least trying to more that direction.  So, how will cognitive components be developed in an agile environment?  I don’t think there will be a separation as Peri predicts.  We have been seeing the opposite trend in practice lately.  One example is DevOps.  Infrastructure management used to be the domain of a separate team that specializes in the that task.  Today, that task has been automated and has become largely just another coding task.  I suspect that however cognitive component development evolves, it will not be a separate task for separate team.  It will be an integrated part of the development process.  But, I know almost nothing about machine learning, so I could be completely wrong.

